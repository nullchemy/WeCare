{"cells":[{"cell_type":"markdown","metadata":{"id":"fI_rEVmCOi8J"},"source":["# Chatbot\n","This notebook aims to host an interactive mental health chatbot for users using DialoGPT and customised retrieval-based logic."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5752,"status":"ok","timestamp":1691759181498,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"HGp0JC-OILMW","outputId":"df8e21e7-b9a6-4a2e-e0a3-f1845dd535b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1691759181509,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"SU1Pv0PxHST9"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'torch'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32me:\\projects\\WeCare\\backend\\chatbot.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Import packages\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"]}],"source":["# Import packages\n","import os\n","import torch\n","import random\n","import pandas as pd\n","from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from IPython.display import Markdown, display\n","from datasets import Dataset"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1691759181511,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"V0b5QQeyPLo5","outputId":"6b7ea4e0-8542-44b5-99fa-1317ea06c5ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory changed\n"]}],"source":["# Change to your own directory\n","try:\n","    os.chdir(\"/content/drive/MyDrive/suicidal-text-detection\")\n","    print(\"Directory changed\")\n","except OSError:\n","    print(\"Error: Can't change the Current Working Directory\")"]},{"cell_type":"markdown","metadata":{"id":"kNjSKmWqNEw3"},"source":["## Define Constants"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1691759181512,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"mewD7JAV3bEn"},"outputs":[],"source":["start_message = \"==== Hello! I am Alex and I am your virtual friend. If you need a listening ear, I'm always here. To end the chat, input 'exit' in the chatbox. ====\"\n","\n","prevention_messages = [\"Are you okay? How long have you been feeling this way?\",\n","                       \"That sounds so painful, and I appreciate you sharing that with me. How can I help?\",\n","                       \"I know things seem bleak now, but it can be hard to see possible solutions when you feel so overwhelmed.\",\n","                       \"I'm concerned about you because I care, and I want to offer support however I can. You can talk to me.\",\n","                       \"I'm always here if you feel like talking.\",\n","                       \"I'm always here to listen, but do you think a therapist could help a little more?\",\n","                       \"Have you thought about talking to a therapist?\",\n","                       \"You can withstand any storm and when you are too tired to stand, I will hold you up. You are never alone.\",\n","                       \"You know I’m always here for you.\",\n","                       \"You’re allowed to have bad days, but remember tomorrow is a brand new day.\",\n","                       \"You’ve got a place here on Earth for a reason.\",\n","                       \"It's okay to have such thoughts but if they become overwhelming, don't keep it to yourself. I am here for you.\",\n","                       \"Everything is a season, and right now you’re in winter. It’s dark and cold and you can’t find shelter, but one day it’ll be summer, and you’ll look back and be grateful you stuck it out through winter.\",\n","                       \"I know you are going through a lot and you’re scared, but you will never lose me.\",\n","                       \"I know it feels like a lot right now. It’s OK to feel that way.\",\n","                       \"Is there anything I can do to make this day go easier for you?\"]\n","\n","helpline_message = \"In times of severe distress where you need to speak with someone immediately, these are suicide hotline services available for you. You will be speaking with volunteers or professionals who are trained to deal with suicide crisis. Samaritans of Singapore (SOS; 24 hours): 1800 221 4444 Mental Health Helpline (24 hours): 6389 2222 Singapore Association for Mental Health (SAMH) Helpline: 1800 283 7019\""]},{"cell_type":"markdown","metadata":{"id":"uX-JLdSobUra"},"source":["## Define Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1691759181512,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"DcL8IXHBNlwH"},"outputs":[],"source":["def printmd(string):\n","    \"\"\"Print text using markdown.\"\"\"\n","\n","    display(Markdown(string))"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1691759181516,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"8LeO0EY5OEJf"},"outputs":[],"source":["def load_tokenizer_and_model(model=\"microsoft/DialoGPT-large\"):\n","  \"\"\"Load tokenizer and model instance for some specific DialoGPT model.\"\"\"\n","\n","  tokenizer = AutoTokenizer.from_pretrained(model)\n","  model = AutoModelForCausalLM.from_pretrained(model)\n","\n","  return tokenizer, model"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":40,"status":"ok","timestamp":1691759181518,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"HIrdkrjkBDtt"},"outputs":[],"source":["def load_suicide_tokenizer_and_model(tokenizer=\"google/electra-base-discriminator\", model=\"Models/electra\"):\n","  \"\"\"Load tokenizer and model instance for suicide text detection model.\"\"\"\n","\n","  suicide_tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n","  suicide_model = AutoModelForSequenceClassification.from_pretrained(model)\n","\n","  return suicide_tokenizer, suicide_model"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":42,"status":"ok","timestamp":1691759181520,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"cckPraXQOX_s"},"outputs":[],"source":["def check_intent(text):\n","  \"\"\"Check if suicidal intent is present in text\"\"\"\n","\n","  global suicide_tokenizer, suicide_model\n","\n","  tokenised_text = suicide_tokenizer.encode_plus(text, return_tensors=\"pt\")\n","\n","  logits = suicide_model(**tokenised_text)[0]\n","\n","  prediction = round(torch.softmax(logits, dim=1).tolist()[0][1])\n","\n","  return prediction"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1691759181523,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"NquvOe-uOKdT"},"outputs":[],"source":["def generate_response(tokenizer, model, chat_round, chat_history_ids):\n","  \"\"\"Generate a response to some user input.\"\"\"\n","\n","  user_input = input(\">> You: \")\n","\n","  if user_input == \"exit\":\n","    raise Exception(\"End of Conversation\")\n","\n","  # Encode user input and End-of-String (EOS) token\n","  new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n","\n","  # Append tokens to chat history\n","  bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_round > 0 else new_input_ids\n","\n","  # Generate response given maximum chat length history of 1250 tokens\n","  chat_history_ids = model.generate(bot_input_ids, max_length=1250, pad_token_id=tokenizer.eos_token_id)\n","\n","  # Print response based on intent\n","  if check_intent(user_input):\n","    printmd(\"*Alex:* {}\".format(random.choice(prevention_messages)))\n","    printmd(\"{}\".format(helpline_message))\n","  else:\n","    printmd(\"*Alex:* {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n","\n","  # Return the chat history ids\n","  return chat_history_ids"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":15195,"status":"ok","timestamp":1691759196674,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"aVEI4qpbNd6i"},"outputs":[{"ename":"NameError","evalue":"name 'AutoTokenizer' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32me:\\projects\\WeCare\\backend\\chatbot.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Initialize chatbot tokenizer and model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tokenizer, model \u001b[39m=\u001b[39m load_tokenizer_and_model()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Initialize chatbot history variable\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m chat_history_ids \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32me:\\projects\\WeCare\\backend\\chatbot.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tokenizer_and_model\u001b[39m(model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/DialoGPT-large\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Load tokenizer and model instance for some specific DialoGPT model.\"\"\"\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/WeCare/backend/chatbot.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m tokenizer, model\n","\u001b[1;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"]}],"source":["# Initialize chatbot tokenizer and model\n","tokenizer, model = load_tokenizer_and_model()\n","\n","# Initialize chatbot history variable\n","chat_history_ids = None"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":652,"status":"error","timestamp":1691759208920,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"Bpp-bhhwA4ol","outputId":"9f4c461c-77a0-4779-80d1-263292dab879"},"outputs":[{"ename":"OSError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/Models/electra/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1196\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     )\n\u001b[0;32m-> 1541\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-64d63267-2bb48ad4342b71305c7c26ed;e2f72007-1a9e-49a7-a678-458d33aec499)\n\nRepository Not Found for url: https://huggingface.co/Models/electra/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-a2f720fc1444>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialise suicide detection tokenizer and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msuicide_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuicide_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_suicide_tokenizer_and_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-23-f9ddf23702ee>\u001b[0m in \u001b[0;36mload_suicide_tokenizer_and_model\u001b[0;34m(tokenizer, model)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msuicide_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0msuicide_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msuicide_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuicide_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch_dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    462\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_or_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trust_remote_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Models/electra is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."]}],"source":["# Initialise suicide detection tokenizer and model\n","suicide_tokenizer, suicide_model = load_suicide_tokenizer_and_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1691759197323,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"khLqZZJJNvLY"},"outputs":[],"source":["def start_chatbot(n=1000):\n","  \"\"\"\n","  Chat with chatbot for n rounds\n","  \"\"\"\n","\n","  global tokenizer, model, chat_history_ids\n","\n","  # input() might not be able to run due to this line, restart the notebook\n","  print(start_message)\n","\n","  # Chat for n rounds\n","  try:\n","    for chat_round in range(n):\n","      chat_history_ids = generate_response(tokenizer, model, chat_round, chat_history_ids)\n","  except Exception as e:\n","    printmd(\"*Alex:* See ya\")"]},{"cell_type":"markdown","metadata":{"id":"yqUssxxRNBH-"},"source":["## Chatbot Execution"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1691759197325,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"kDyPEcKKOQPT"},"outputs":[],"source":["start_chatbot()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1691759197325,"user":{"displayName":"Dennis Kibet","userId":"11744330112719392161"},"user_tz":-180},"id":"WX_WaMfxbaia"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
